{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww14200\viewh14600\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 Plot 1 (Top Left):\
1. No labels on axis (which one is TPR vs. FPR)\
2. Scales of axis should be between 0 and 1, current representation may be a bit misleading of relationship.\
3. Why are same models giving different results. E.g. What\'92s the difference between Bayesian 2019-10-19 vs. Bayesian 2019-11-31\
4. In general comparing models using TPR vs FPR does not give us a good way to compare inter model performance since it is mainly a tradeoff. We can plot ROC per model (varying classification threshold) and compare AUC of each model for best performance. If we mainly interested in a single metric (i.e. FPR or TPR), using precision or recall may be more appropriate. \
\
Plot 2 (Top Right): \
1. What does each data point represent? Would b good to have model labels next to each point.\
2. \'93Inference\'94 is spelled incorrectly.\
3. Inconsistent capitalization on x-axis label. \
4. Scale of FPR should be between 0 and 1.\
5. In general FPR might not be a great metric to use here. For example, I can make a really fast classifier that predicts every example as the negative class. We would have a FPR of 0 and inference time would be very fast but wouldn\'92t be a very good model. \
\
\
Table (Bottom):\
1. What type of data are the models being evaluated on (i.e. train, test, val)? \
2. Total # of observations are inconsistent. All models should be evaluated against same set to have apples-to-apples comparison. \
3. \'93Regresion\'94 is spelled incorrectly.\
4. What are the differences between the models? Different hyper parameters, different training schemes, etc.?\
5. Breakdown of TP, FP, TN, and FN for \'93SVM 2019-10-25\'94 do not add up (30 + 30 + 30 + 30 = 120) to the Total Observations.\
6. TPR column header has formula but FPR column header does not. \
7. Time per observation does not have units in column header.\
8. Inconsistent \'93Time per observation\'94 for row \'93SVM 2019-11-14\'94 -> Total Runtime / Total Observations = 20/80 = 0.25 rather than 0.2\
9. Again, more generally, I think using metrics like F1 and ROC AUC would be more effective metrics for comparing models against one another. \
10. \'93Inferrence\'94 spelled incorrectly in \'93Model Inferrence Time\'94 column header. \
}